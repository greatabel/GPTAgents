# test_autogen_official.py
"""
ä½¿ç”¨å®˜æ–¹ AutoGen (pyautogen) æµ‹è¯• DeepSeek å·¥å…·è°ƒç”¨

å‰æï¼š
1. pip install pyautogen
2. ä»£ç†æœåŠ¡å™¨è¿è¡Œ: python deepseek_proxy_server.py
3. OAI_CONFIG_LIST æ–‡ä»¶å­˜åœ¨
"""

from autogen import ConversableAgent, UserProxyAgent, config_list_from_json

print("="*60)
print("ğŸ§ª æµ‹è¯•å®˜æ–¹ AutoGen with DeepSeek")
print("="*60)
print()

# åŠ è½½é…ç½®æ–‡ä»¶
config_list = config_list_from_json(env_or_file="OAI_CONFIG_LIST")

print(f"âœ… åŠ è½½äº† {len(config_list)} ä¸ªé…ç½®:")
for i, cfg in enumerate(config_list, 1):
    print(f"   {i}. æ¨¡å‹: {cfg['model']}, URL: {cfg.get('base_url', 'default')}")
print()

# åˆ›å»ºä½¿ç”¨ LLM çš„æ™ºèƒ½ä½“
assistant = ConversableAgent(
    name="assistant",
    llm_config={"config_list": config_list}
)

# åˆ›å»ºä»£è¡¨ç”¨æˆ·çš„æ™ºèƒ½ä½“
user_proxy = UserProxyAgent(
    name="user",
    code_execution_config={
        "work_dir": "working",
        "use_docker": False,
    },
    human_input_mode="ALWAYS",  # æ€»æ˜¯ç­‰å¾…ç”¨æˆ·è¾“å…¥
    is_termination_msg=lambda x: x.get("content", "").rstrip().endswith("TERMINATE"),
)

print("ğŸš€ å¼€å§‹å¯¹è¯...")
print()

# å¼€å§‹å¯¹è¯
user_proxy.initiate_chat(
    assistant, 
    message="è¯·ä»‹ç»ä¸€ä¸‹ Python çš„ä¸»è¦ç‰¹ç‚¹ã€‚å›ç­”å®Œåè¯´ TERMINATEã€‚"
)

print()
print("="*60)
print("âœ… å¯¹è¯å®Œæˆ")
print("="*60)